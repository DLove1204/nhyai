{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crnn ocr 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "# from warpctc_pytorch import CTCLoss\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nhydev/github/chineseocr/chineseocr/train/ocr\n"
     ]
    }
   ],
   "source": [
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据软连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /tmp/ICDR2019/train_images ../data/ocr/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "from train.ocr.dataset import PathDataset,randomSequentialSampler,alignCollate\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "roots = glob('./train/data/ocr/*/*.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./train/data/ocr/0/4.jpg', './train/data/ocr/0/7.jpg', './train/data/ocr/0/5.jpg', './train/data/ocr/0/3.jpg', './train/data/ocr/0/2.jpg', './train/data/ocr/0/6.jpg', './train/data/ocr/0/1.jpg', './train/data/ocr/0/0.jpg']\n"
     ]
    }
   ],
   "source": [
    "print (roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nhydev/github/chineseocr/chineseocr\n"
     ]
    }
   ],
   "source": [
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练字符集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetChinese = '1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainP,testP = train_test_split(roots,test_size=0.1)##此处未考虑字符平衡划分\n",
    "traindataset = PathDataset(trainP,alphabetChinese)\n",
    "testdataset = PathDataset(testP,alphabetChinese)\n",
    "\n",
    "batchSize = 32\n",
    "workers = 1\n",
    "imgH = 32\n",
    "imgW = 280\n",
    "keep_ratio = True\n",
    "cuda = True\n",
    "ngpu = 1\n",
    "nh =256\n",
    "sampler = randomSequentialSampler(traindataset, batchSize)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    traindataset, batch_size=batchSize,\n",
    "    shuffle=False, sampler=None,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=alignCollate(imgH=imgH, imgW=imgW, keep_ratio=keep_ratio))\n",
    "\n",
    "train_iter = iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "from crnn.network_torch import CRNN\n",
    "from config import ocrModel,LSTMFLAG,GPU\n",
    "model = CRNN(32, 1, len(alphabetChinese)+1, 256, 1,lstmFlag=LSTMFLAG)\n",
    "model.apply(weights_init)\n",
    "preWeightDict = torch.load(ocrModel,map_location=lambda storage, loc: storage)##加入项目训练的权重\n",
    "\n",
    "modelWeightDict = model.state_dict()\n",
    "\n",
    "for k, v in preWeightDict.items():\n",
    "            name = k.replace('module.','') # remove `module.`\n",
    "            if  'rnn.1.embedding' not in name:##不加载最后一层权重\n",
    "                 modelWeightDict[name] = v\n",
    "            \n",
    "model.load_state_dict(modelWeightDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU(inplace)\n",
       "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace)\n",
       "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU(inplace)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace)\n",
       "  )\n",
       "  (rnn): Sequential(\n",
       "    (0): BidirectionalLSTM(\n",
       "      (rnn): LSTM(512, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): BidirectionalLSTM(\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=63, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##优化器\n",
    "from crnn.util import strLabelConverter\n",
    "lr = 0.1\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "converter = strLabelConverter(''.join(alphabetChinese))\n",
    "# criterion = CTCLoss()\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from train.ocr.dataset import resizeNormalize\n",
    "from crnn.util import loadData\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0])##转换为多GPU训练模型\n",
    "    image = image.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainBatch(net, criterion, optimizer,cpu_images, cpu_texts):\n",
    "    #data = train_iter.next()\n",
    "    #cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    \n",
    "    loadData(text, t)\n",
    "    loadData(length, l)\n",
    "    preds = net(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    net.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(im):\n",
    "    \"\"\"\n",
    "    预测\n",
    "    \"\"\"\n",
    "    image = im.convert('L')\n",
    "    scale = image.size[1]*1.0 / 32\n",
    "    w = image.size[0] / scale\n",
    "    w = int(w)\n",
    "    transformer = resizeNormalize((w, 32))\n",
    "    \n",
    "    image = transformer(image)\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "    image = image.view(1, *image.size())\n",
    "    image = Variable(image)\n",
    "    preds = model(image)\n",
    "    _, preds = preds.max(2)\n",
    "    preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)]))\n",
    "    sim_pred = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "    return sim_pred\n",
    "   \n",
    "   \n",
    "def val(net, dataset, max_iter=100):\n",
    "\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad = False\n",
    "    net.eval()\n",
    "    i = 0\n",
    "    n_correct = 0\n",
    "    N = len(dataset)\n",
    "    \n",
    "    max_iter = min(max_iter, N)\n",
    "    for i in range(max_iter):\n",
    "        im,label = dataset[np.random.randint(0,N)]\n",
    "        if im.size[0]>1024:\n",
    "            continue\n",
    "        \n",
    "        pred = predict(im)\n",
    "        if pred.strip() ==label:\n",
    "                n_correct += 1\n",
    "\n",
    "    accuracy = n_correct / float(max_iter )\n",
    "    return accuracy\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.ocr.generic_utils import Progbar\n",
    "##进度条参考 https://github.com/keras-team/keras/blob/master/keras/utils/generic_utils.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冻结预训练模型层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0/10\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed6520dfe05b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mcurAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurAcc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "nepochs = 10\n",
    "acc  = 0\n",
    "\n",
    "interval = len(train_loader)//2##评估模型\n",
    "\n",
    "            \n",
    "for i in range(nepochs):\n",
    "    print('epoch:{}/{}'.format(i,nepochs))\n",
    "    n = len(train_loader)\n",
    "    pbar = Progbar(target=n)\n",
    "    train_iter = iter(train_loader)\n",
    "    loss = 0\n",
    "    for j in range(n):\n",
    "        for p in model.named_parameters():\n",
    "            p[1].requires_grad = True\n",
    "            if 'rnn.1.embedding' in p[0]:\n",
    "               p[1].requires_grad = True\n",
    "            else:\n",
    "                p[1].requires_grad = False##冻结模型层\n",
    "\n",
    "        model.train()\n",
    "        cpu_images, cpu_texts = train_iter.next()\n",
    "        cost = trainBatch(model, criterion, optimizer,cpu_images, cpu_texts)\n",
    "\n",
    "        loss += cost.data.cpu().numpy()\n",
    "        \n",
    "        if (j+1)%interval==0:\n",
    "            curAcc = val(model, testdataset, max_iter=1024)\n",
    "            if curAcc>acc:\n",
    "                acc = curAcc\n",
    "                torch.save(model.state_dict(), 'train/ocr/modellstm.pth')\n",
    "            \n",
    "               \n",
    "        pbar.update(j+1,values=[('loss',loss/((j+1)*batchSize)),('acc',acc)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 释放模型层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10\n",
    "#acc  = 0\n",
    "\n",
    "interval = len(train_loader)//2##评估模型\n",
    "\n",
    "            \n",
    "for i in range(10,10+nepochs):\n",
    "    print('epoch:{}/{}'.format(i,nepochs))\n",
    "    n = len(train_loader)\n",
    "    pbar = Progbar(target=n)\n",
    "    train_iter = iter(train_loader)\n",
    "    loss = 0\n",
    "    for j in range(n):\n",
    "        for p in model.named_parameters():\n",
    "            p[1].requires_grad = True\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        cpu_images, cpu_texts = train_iter.next()\n",
    "        cost = trainBatch(model, criterion, optimizer,cpu_images, cpu_texts)\n",
    "\n",
    "        loss += cost.data.numpy()\n",
    "        \n",
    "        if (j+1)%interval==0:\n",
    "            curAcc = val(model, testdataset, max_iter=1024)\n",
    "            if curAcc>acc:\n",
    "                acc = curAcc\n",
    "                torch.save(model.state_dict(), 'train/ocr/modellstm.pth')\n",
    "            \n",
    "               \n",
    "        pbar.update(j+1,values=[('loss',loss/((j+1)*batchSize)),('acc',acc)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N  = len(testdataset)\n",
    "im,label = testdataset[np.random.randint(0,N)]\n",
    "pred = predict(im)\n",
    "print('true:{},pred:{}'.format(label,pred))\n",
    "im\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
